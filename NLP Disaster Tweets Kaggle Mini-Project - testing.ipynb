{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: NLP Disaster Tweets Kaggle Mini-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description of the problem and data (5 pts)\n",
    "\n",
    "*Briefly describe the challenge problem and NLP. Describe the size, dimension, structure, etc., of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data (15 pts)\n",
    "\n",
    "*Show a few visualizations like histograms. Describe any data cleaning procedures. Based on your EDA, what is your plan of analysis?*\n",
    "\n",
    "Checking the general structure of the data and potential duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df):\n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i, 'word_count'] = len(row.text.split())\n",
    "        txt = df.loc[i, 'text']\n",
    "        txt = re.sub(r'https?://\\S+|www.\\S+', '', txt) # Remove URLs\n",
    "        txt = re.sub(r'[^a-z0-9A-Z\\s]', '', txt) # Remove numbers\n",
    "        # txt = txt.lower()\n",
    "        df.loc[i, 'text'] = txt\n",
    "    df['word_count'] = df['word_count'].astype(int)\n",
    "    all_text = ' '.join(df.text)\n",
    "    unique_words = len(set(all_text.split()))\n",
    "    return unique_words\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_unique_words = count_words(train_df)\n",
    "test_unique_words = count_words(test_df)\n",
    "\n",
    "all_text = ' '.join(((pd.concat([train_df,test_df], axis=0)).text.values))\n",
    "all_unique_words = len(set(all_text.split()))\n",
    "\n",
    "\n",
    "print('\\n' + 40*'*' + ' Train dataset ' + 40*'*')\n",
    "train_df.info()\n",
    "print('\\nNumerical statistics:\\n', train_df.describe())\n",
    "print('\\n', train_df.head(4), '\\n')\n",
    "# print('\\n', train_df.tail(3))\n",
    "print('Number of duplicated rows:', np.sum(train_df.duplicated()))\n",
    "print('Number of duplicated texts:', np.sum(train_df.duplicated(subset='text')))\n",
    "print('Longest tweet has', np.max(train_df.word_count), 'words.')\n",
    "print('Unique words in the dataset:', train_unique_words)\n",
    "print('Target values:', pd.unique(train_df.target))\n",
    "y_split = round(100 * np.sum(train_df.target == 1)/len(train_df.target))\n",
    "print('Target split: \\n1 (disaster) =', y_split, '%\\n0 (not disaster) =', 100-y_split, '%')\n",
    "\n",
    "print('\\n' + 40*'*' + ' Test dataset ' + 40*'*')\n",
    "test_df.info()\n",
    "print('\\nNumerical statistics:\\n', test_df.describe())\n",
    "print('\\n', test_df.head(4), '\\n')\n",
    "# print('\\n', test_df.tail(3))\n",
    "print('Number of duplicated rows:', np.sum(test_df.duplicated()))\n",
    "print('Number of duplicated texts:', np.sum(test_df.duplicated(subset='text')))\n",
    "print('Longest tweet has', np.max(test_df.word_count), 'words.')\n",
    "print('Unique words in the dataset:', test_unique_words)\n",
    "\n",
    "\n",
    "sns.histplot(train_df, x='word_count', bins=30, stat='percent')\n",
    "sns.histplot(test_df, x='word_count', bins=30, stat='percent')\n",
    "plt.grid(axis='y')\n",
    "plt.title('Histogram of approximate word counts in the data')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size with duplicated texts:', len(train_df))\n",
    "train_df.drop_duplicates(subset='text', inplace=True)\n",
    "print('Size without duplicated texts:', len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture (25 pts)\n",
    "\n",
    "*Describe your model architecture and reasoning for why you believe that specific architecture would be suitable for this problem.*\n",
    "\n",
    "*Since we did not learn NLP-specific techniques such as word embeddings in the lectures, we recommend looking at Kaggle tutorials, discussion boards, and code examples posted for this challenge.  You can use any resources needed, but make sure you “demonstrate” you understood by including explanations in your own words. Also importantly, please have a reference list at the end of the report.*\n",
    "\n",
    "*There are many methods to process texts to matrix form (word embedding), including TF-IDF, GloVe, Word2Vec, etc. Pick a strategy and process the raw texts to word embedding. Briefly explain the method(s) and how they work in your own words.*\n",
    "\n",
    "*Build and train your sequential neural network model (You may use any RNN family neural network, including advanced architectures LSTM, GRU, bidirectional RNN, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "max_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=2,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=None,\n",
    "    vocabulary=None,\n",
    "    idf_weights=None,\n",
    "    sparse=False,\n",
    "    ragged=False,\n",
    "    encoding=\"utf-8\",\n",
    "    name=None,\n",
    ")\n",
    "my_vectorizer.adapt(train_df['text'])\n",
    "x_train = my_vectorizer(train_df['text'])\n",
    "\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = train_df.target\n",
    "\n",
    "print('Shape:', x_train.shape)\n",
    "print('Min and max:', np.min(x_train), np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:,['text','keyword','location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.95, min_df=2, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "# my_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.95, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 1), stop_words='english')\n",
    "# my_vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "my_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.95, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', max_features=max_features)\n",
    "my_vectorizer.fit(train_df.text)\n",
    "x_data = my_vectorizer.transform(train_df['text'])\n",
    "x_train = x_data\n",
    "y_train = train_df.target\n",
    "\n",
    "print('Shape of x_train:', x_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "ftr_names = my_vectorizer.get_feature_names_out()\n",
    "print('Length of ftr_names:',len(ftr_names))\n",
    "print('ftr_names:', my_vectorizer.get_feature_names_out())\n",
    "\n",
    "x_test = my_vectorizer.transform(test_df['text'])\n",
    "print('Shape of X_test:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensor Flow data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(train_df.text.values)\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "max_features = 10000  # Maximum vocabulary size\n",
    "sequence_length = 100  # Maximum sequence length\n",
    "\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the layer to the dataset\n",
    "vectorize_layer.adapt(dataset)\n",
    "\n",
    "# Apply the TextVectorization layer to the dataset\n",
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)\n",
    "\n",
    "vectorized_dataset = dataset.map(vectorize_text)\n",
    "\n",
    "# Example: Print the first 5 tokenized sequences\n",
    "for vectorized_text in vectorized_dataset.take(5):\n",
    "    print(vectorized_text.numpy())\n",
    "\n",
    "\n",
    "target_ds = tf.data.Dataset.from_tensor_slices(train_df.target.values)\n",
    "\n",
    "train_ds = tf.data.Dataset.zip((vectorized_dataset, target_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df.text.values, train_df.target.values))\n",
    "test_ds =  tf.data.Dataset.from_tensor_slices((test_df.text.values))\n",
    "all_txt = pd.concat([train_df.text, test_df.text], axis=0)\n",
    "train_test_ds = tf.data.Dataset.from_tensor_slices((all_txt.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in test_ds.take(5):\n",
    "    tf.print(txt.numpy()[ :50])\n",
    "for txt, trg in train_ds.take(5):\n",
    "    tf.print(trg, '\\t', txt.numpy()[ :50])\n",
    "for txt in train_test_ds.take(5):\n",
    "    tf.print(txt.numpy()[ :50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_count = len(train_df)\n",
    "val_size = int(text_count * 0.2)\n",
    "print('Validation size:', val_size)\n",
    "train_ds = train_ds.shuffle(text_count, reshuffle_each_iteration=False)\n",
    "val_ds = train_ds.take(val_size)\n",
    "train_ds = train_ds.skip(val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=all_unique_words, split=' ', oov_token='<OOV>')\n",
    "\n",
    "# Fit the tokenizer on the sentences\n",
    "tokenizer.fit_on_texts(all_txt)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index['a'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(all_txt)\n",
    "print(len(sequences))\n",
    "\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
    "print(padded_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_txt))\n",
    "print(word_index['hello'])\n",
    "print(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_train(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn_train(text, label):\n",
    "    return tf.py_function(encode_train, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "def encode_unseen(text_tensor):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, 1\n",
    "\n",
    "def encode_map_fn_unseen(text):\n",
    "    return tf.py_function(encode_unseen, inp=[text], Tout=(tf.int64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = tfds.deprecated.text.Tokenizer()\n",
    "token_counts = Counter()\n",
    "\n",
    "for example in test_ds:\n",
    "    tokens = tokeniser.tokenize(example.numpy())\n",
    "    token_counts.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(test_ds)\n",
    "example_string = \"This is an example\"\n",
    "print(f\"Exmaple String: {example_string}\")\n",
    "print(f\"Encoded String: {encoder.encode(example_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_train(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text =  tokenizer.texts_to_sequences(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_test(text_tensor):\n",
    "    # text = text_tensor.numpy()[0]\n",
    "    # text = tf.strings.reduce_join(text_tensor, separator=' ').numpy().decode('utf-8')\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    encoded_text =  tokenizer.texts_to_sequences(text)\n",
    "    return encoded_text\n",
    "\n",
    "def encode_map_fn_train(text, label):\n",
    "    return tf.py_function(encode_train, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "def encode_map_fn_test(text):\n",
    "    tokens = tf.py_function(encode_test, inp=[text], Tout=(tf.int64))\n",
    "    tokens = tf.convert_to_tensor(tokens)\n",
    "    return tokens\n",
    "\n",
    "def _fixup_shape(text, label):\n",
    "    text.set_shape([])\n",
    "    label.set_shape([])\n",
    "    return text, label\n",
    "\n",
    "def _fixup_test_shape(text):\n",
    "    text.set_shape([])\n",
    "    return text\n",
    "\n",
    "# train_ds = train_ds.map(encode_map_fn_train)\n",
    "# val_ds = val_ds.map(encode_map_fn_train)\n",
    "test_ds = test_ds.map(encode_map_fn_test)\n",
    "# test_ds = test_ds.map(_fixup_test_shape)\n",
    "test_ds = test_ds.padded_batch(32, padded_shapes=([-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in test_ds.take(5):\n",
    "    tf.print(txt.numpy()[ :50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example Sequences and their length:\\n\")\n",
    "example = train_ds.take(8)\n",
    "for ex in example:\n",
    "    print(f\"Individual Size: {ex[0].shape}\")\n",
    "print(\"Batched examples and the sequence length:\\n\")\n",
    "batched_example = example.padded_batch(4, padded_shapes=([-1], []))\n",
    "for batch in batched_example:\n",
    "    print(f\"Batch dimension: {batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.padded_batch(32, padded_shapes=([-1], []))\n",
    "tweets_valid = tweets_valid.padded_batch(32, padded_shapes=([-1], []))\n",
    "tweets_unseen_batched = tweets_unseen_map.padded_batch(32, padded_shapes=([-1], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "\n",
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 LSTMs\n",
    "x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.LSTM(128)(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 LSTMs\n",
    "# x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.GRU(64, return_sequences=True)(x)\n",
    "x = layers.GRU(64)(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.kaggle.com/code/anmolstha/disaster-tweets-simple-rnn-implementation\n",
    "# Long Short Term Memory network.\n",
    "\n",
    "# We need sequential model to process sequence of text data\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Embedding(input_dimension, output_dimension,embeddings_initializer = initialize the embedding matrix we created, trainable = do not train)\n",
    "embedding= layers.Embedding(max_features, 100, trainable=False)\n",
    "# Adding Embedding Layer\n",
    "model.add(embedding)\n",
    "\n",
    "# Drops 40% of entire row\n",
    "model.add(layers.SpatialDropout1D(0.4))\n",
    "\n",
    "# Recurrent Layer LSTM(dimensionality of the output space, dropout = 20%, recurrent_dropout = 20%) \n",
    "model.add(layers.GRU(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(layers.GRU(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(layers.GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Decide what we are going to output Dense(units, activation function)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model compile(loss = binary crossentropy, use Adam(adaptive moment estimation) optimizer with learning rate 1e-3,evaluate based on accuracy)\n",
    "model.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "max length = 1000\n",
    "\n",
    "3rd layer 0.5391\n",
    "\n",
    "256 dim 0.5391\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_ds, batch_size=32, epochs=1, validation_data=val_ds)\n",
    "# history = model.fit(train_ds, batch_size=32, epochs=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis (35 pts)\n",
    "\n",
    "*Run hyperparameter tuning, try different architectures for comparison, apply techniques to improve training or performance, and discuss what helped.*\n",
    "\n",
    "*Includes results with tables and figures. There is an analysis of why or why not something worked well, troubleshooting, and a hyperparameter optimization procedure summary.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (15 pts)\n",
    "\n",
    "*Discuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
